---

### VP抽出ロジックの現状の問題点

1.  **助動詞の独立したVP検出:**
    *   `will`, `has`, `been`, `is` といった助動詞が、主動詞と結合せず、単独でVPとして検出されてしまう。
    *   本来は、これらの助動詞は主動詞と一体となって一つのVPを形成すべき。

2.  **VPの範囲の不正確さ:**
    *   動詞の後に続く目的語、補語、前置詞句、副詞句などがVPの範囲に正しく含まれていない。
    *   例: `jumps over the lazy dog` のうち `over the lazy dog` がVPに含まれない。
    *   例: `is running quickly in the park` のうち `in the park` がVPに含まれない。

3.  **副詞句 (ADVP) の範囲の不正確さ:**
    *   `very` のような修飾副詞が、修飾する副詞 (`hard`) と結合せず、単独でADVPとして検出されてしまう。
    *   本来は、`very hard` のように一体となって一つのADVPを形成すべき。

### VP抽出ロジック改善のためのロードマップ

これらの問題点を解決し、より正確なVPおよびADVP抽出を実現するためのロードマップを提案します。

#### フェーズ2.1: VP抽出ロジックの強化 (優先度: 最も高い)

1.  **`get_verb_phrase_tokens` 関数の依存関係網羅性の向上:**
    *   現在 `get_verb_phrase_tokens` 関数内で考慮されている依存関係タイプ (`aux`, `auxpass`, `dobj`, `iobj`, `attr`, `acomp`, `xcomp`, `ccomp`, `advcl`, `prt`, `agent`, `oprd`, `neg`, `prep`) に加えて、VPを構成する可能性のある他の重要な依存関係タイプ（例: `oprd` (目的語補語), `agent` (動作主), `advmod` (副詞修飾語) など）を網羅的に追加します。
    *   特に、動詞を修飾する副詞句 (`advmod`) や、動詞の補語となる句 (`xcomp`, `ccomp`)、そして動詞に付随する前置詞句 (`prep` の子孫全体) をVPの範囲に含めるロジックを強化します。
2.  **再帰的探索の深化と句の境界の正確な特定:**
    *   `get_verb_phrase_tokens` 関数が、収集したトークンの `subtree` をより効果的に利用し、句の開始と終了を正確に特定するようにロジックを調整します。
    *   特に、句の内部に別の句が含まれる場合（例: VP内のPP）に、その内部の句全体をVPの範囲に含めるようにします。
3.  **助動詞と主動詞の結合ロジックの強化:**
    *   `will`, `has been`, `is running` のように、助動詞が主動詞と一体となってVPを形成するように、`get_verb_phrase_tokens` 関数内で助動詞と主動詞の関係をより厳密に追跡するロジックを導入します。

#### フェーズ2.2: ADVP抽出ロジックの強化 (優先度: 高)

1.  **`get_adverb_phrase_tokens` 関数の実装 (新規または既存ロジックの改善):**
    *   `ADV` (副詞) を中心に、それを修飾する他の副詞 (`advmod`) や、副詞句を構成する可能性のある要素を再帰的に探索し、ADVPを正確に特定するロジックを実装します。
    *   `very hard` のようなケースで、`very` と `hard` が一体となってADVPとして検出されるようにします。

#### フェーズ2.3: 句の重複とネストの最終整理 (優先度: 中)

1.  **`remove_subsets` 関数の再評価と適用:**
    *   `analyzer.py` の `remove_subsets` 関数は現在使用されていませんが、句の抽出ロジックが強化された後、より大きな句の中に含まれる小さな句を適切に排除するために、この関数を再評価し、必要に応じて適用します。
    *   ただし、`implementation_plan.md` の「ネストした句の表示も考慮する」という要件とのバランスを考慮し、表示側でのネスト表現と、データとしての重複排除の役割を明確にします。

#### フェーズ2.4: テストと検証 (継続的)

1.  **多様な例文での徹底的なテスト:**
    *   特に、助動詞を含む文、動詞の後に目的語や前置詞句が続く文、副詞が他の副詞を修飾する文など、VP/ADVP抽出の課題となるパターンを網羅的にテストします。
    *   期待される出力と実際の出力を比較し、ロジックの改善効果を検証します。

---

### 実装順序の提案

以下の順序で実装を進めるのが最も効率的かつ確実です。

1.  **【最優先】VP (動詞句) および ADVP (副詞句) 抽出ロジックの抜本的改善**
    *   **理由:** 現在の課題の根幹であり、正確な句構造解析を実現するための基礎となります。ここが改善されないと、後続の重複排除や表示の改善も意味をなさなくなります。

2.  **【並行実施】テスト駆動開発 (TDD) のアプローチによる検証**
    *   **理由:** ロジックの改善が正しく行われているか、また新たな問題（デグレード）が発生していないかを常に確認しながら進めるためです。具体的な課題となっている例文をテストケースとして追加し、それをパスするように開発を進めます。

3.  **【ロジック改善後】句の重複排除とネスト構造の表示**
    *   **理由:** 正確な句が抽出できるようになった後、それらをどのように整理し、ユーザーに分かりやすく見せるか、という表示層の課題に取り組みます。

---

### 各ステップの具体的な構想

#### 1. VP/ADVP抽出ロジックの改善 (`analyzer.py`)

*   **方針:** 依存関係ツリーをより深く、網羅的に探索することで、句の構成要素を「かたまり」として正確に捉えます。
*   **対象関数:** `get_verb_phrase_tokens`, `get_adverb_phrase_tokens`
*   **具体的な改善策:**
    1.  **助動詞と主動詞の結合:**
        *   動詞トークンを見つけたら、`aux` (助動詞) や `auxpass` (受動態の助動詞) の依存関係を持つ子トークンをすべて遡って収集し、一つのVPに含めます。
    2.  **目的語・補語・修飾語の網羅:**
        *   動詞に直接接続される `dobj` (直接目的語), `prep` (前置詞), `advmod` (副詞修飾語), `acomp` (形容詞補語), `xcomp` (開いた節補語) などの子トークンを見つけます。
        *   それらの子トークンだけでなく、**その子孫 (subtree) 全て**を再帰的に探索し、句の範囲に含めます。これにより、「jumps `over the lazy dog`」のような前置詞句全体がVPの一部として正しく認識されるようになります。
    3.  **ADVPの範囲特定:**
        *   副詞トークンを見つけたら、それを修飾する `advmod` の依存関係を持つ他の副詞（例: `very`）を探索し、まとめて一つのADVPとします。

#### 2. テストと検証 (`tests/test_analyzer.py`)

*   **方針:** 課題として挙げられている例文を元に、具体的なテストケースを作成します。
*   **追加するテストケースの例:**
    *   `"I will have been running quickly."` という文で、`['will', 'have', 'been', 'running', 'quickly']` が一つのVPとして抽出されることを検証するテスト。
    *   `"He jumps over the lazy dog."` という文で、`['jumps', 'over', 'the', 'lazy', 'dog']` が一つのVPとして抽出されることを検証するテスト。
    *   `"She works very hard."` という文で、`['very', 'hard']` が一つのADVPとして抽出されることを検証するテスト。

#### 3. 句の重複とネストの整理 (`app.py` と `analyzer.py`)

*   **方針:** まず解析器(`analyzer.py`)側で全ての可能性のある句を抽出し、その後、表示側(`app.py`)で包含関係にある句を階層的に（ネストして）表示します。単純に `remove_subsets` で削除すると、「VPの中の前置詞句(PP)」のような有益な情報まで消えてしまうため、このアプローチを取ります。
*   **具体的な実装:**
    1.  **`analyzer.py`:** `remove_subsets` 関数は**使用せず**、抽出された全ての句（重複や包含関係を許容）を `app.py` に渡します。
    2.  **`app.py` (`display_chunks` 関数):**
        *   受け取った句のリストを、開始位置と長さでソートします。
        *   HTMLとCSSを工夫し、句のハイライト表示に `padding` や `margin` を適用することで、包含関係にある句が内側にインデントされて表示されるように見せます。
        *   あるいは、背景色を半透明にし、色が重なることでネストを表現する方法も考えられます。